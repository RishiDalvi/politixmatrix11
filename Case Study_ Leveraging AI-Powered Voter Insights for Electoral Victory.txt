Case Study: Leveraging AI-Powered Voter Insights for Electoral Victory—A Longitudinal Analysis




Executive Summary: The Analytical Edge in Modern Campaigns


Forget the narrative that elections are won by one viral deepfake or a flash-in-the-pan technology. When you dig into the data, the truth is far more disciplined. Quantifiable electoral success today hinges on something much less glamorous: the masterful integration of advanced predictive analytics and robust data infrastructure into the campaign’s very DNA. The goal is simple: optimize resource allocation and supercharge the crucial Get Out The Vote (GOTV) efforts. The application of Artificial Intelligence (AI) in politics has evolved across three distinct, fascinating generations: the cold, hard operational efficiency of 2012, the deep-seated, psychographic persuasion tactics of 2016, and the current, chaotic threat of generative disruption in the 2024 cycle.
While newer techniques, like psychographic microtargeting and Generative AI (GenAI), offer potent capabilities for micro-persuasion and large-scale deception, their actual mass persuasive impact often remains secondary to structural factors, superior data integration, and relentless operational discipline.1 The most significant modern impact of GenAI isn't persuasion; it's the scalable, existential threat of disinformation, voter suppression, and the systematic erosion of trust in the electoral process. Consequently, the case studies that truly demonstrate a competitive advantage revolve around a superior data foundation and predictive modeling, not necessarily the more controversial psychological manipulation or disruptive deception.
________________


Section 1: Conceptual Framework and Definitions




1.1 Defining AI-Powered Voter Insights: Beyond the Hype


To navigate this landscape, we must first agree on the language. When we talk about ‘AI-Powered Voter Insights,’ we’re moving past mere hype and distinguishing key tools. "Big Data Analytics" refers to the traditional, yet essential, statistical modeling, clustering, and segmentation of the electorate. "Machine Learning/AI" represents a significant leap, employing sophisticated algorithms to create predictive models—for example, determining the probability of a voter turning out or being swayed by a specific message.2 Finally, the headline-grabbing "Generative AI" involves sophisticated tools capable of crafting highly realistic, novel content, such as deepfakes or personalized ad copy.2
The foundation of all AI-powered insight is, inevitably, high-quality data. Campaigns start with publicly available records—voter registration, party affiliation, address, and participation history—to build a baseline understanding of the electorate.3 These records provide a rough idea of whether a person will vote and their partisan leanings. Crucially, campaigns then augment this public data using extensive digital streams and commercial databases—the voter’s "digital trail"—to build highly detailed profiles.3 This synthesis allows strategists to categorize voters into broad strategic buckets such as "supporter," "non-supporter," or "undecided," providing the starting point for targeted engagement.3


1.2 The Core Goal: Maximizing Probability of Victory


Campaign analytics operates under one singular, unforgiving objective: maximizing the probability of victory.5 To achieve this, analytical models optimize across two primary vectors: Turnout (mobilization of known supporters who might otherwise abstain) and Persuasion (shifting the preferences of undecided or leaning voters). Every dollar, minute, and volunteer hour is rigorously evaluated based on how many votes it is likely to generate and at what cost.
Data-savvy campaigns perform continuous cost-benefit analyses using predictive models, which dictate the optimal allocation of finite and scarce resources—time, money, staff, and volunteers.5 The historical shift in political campaigning is profound: from agencies relying on mass communication (like television ads) to becoming sophisticated logistical and computational operations.5 This shift, which began in earnest in the 2008-2012 era, proves that the true measure of analytical success lies in operational efficiency and the structural integration of data systems, rather than simply generating viral communication. This structural advantage accrues to campaigns that embrace high-level technical organization.


1.3 Regulatory Friction and the Information Fiduciary Concept


A major systemic issue fueling the ethical debate around political AI is the unsettling reality that campaigns exploit the same invasive tricks used by behavioral advertisers, pulling in data from countless online and offline sources to create comprehensive profiles.3 This process enables highly tailored content but comes at the high price of unbridled access to personal data.4 The increasing use of novel analytical techniques to reveal subtle patterns in human behavior creates new ethical challenges regarding consumer manipulation and algorithmic discrimination.4
The legal discussion regarding data privacy reform, particularly in the United States, is constrained by the necessity of reconciling proposed reforms with First Amendment protections.4 Since regulation often lags technological capability, a critical third-order policy solution involves addressing the core data supply issue fueling both legitimate targeting and questionable psychographic practices. This solution centers on defining online service providers as "information fiduciaries." This designation would impose special duties on platforms concerning the information they obtain from users, thereby creating a necessary legal lever to safeguard data usage in political campaigns while navigating existing constitutional limits on political expression.4
________________


Section 2: Case Study I: Operationalizing Victory—The Obama 2012 Model


If the 2012 presidential re-election campaign of Barack Obama proved anything, it’s that a war room of engineers can be more decisive than any cable news pundit. This is the seminal case study illustrating how superior data integration and predictive analytics, rather than controversial persuasive content, provided a decisive competitive advantage. This victory was achieved through meticulous technical architecture and rigorous operational testing.


2.1 Project Narwhal: Strategic Data Integration and Predictive Analytics


Project Narwhal—that was the program’s name—was the advanced computer system developed by the 2012 Obama campaign. Developed by a dedicated staff poached from Silicon Valley giants like Google, Twitter, and Facebook, Narwhal was intended to merge and link previously separate repositories of voter information.6 This linkage meant that all data gathered about an individual voter—whether through field operations, internal polls, or digital engagement—was instantaneously available to all campaign arms, ensuring coordination and data uniformity across the entire enterprise.6
The success of Narwhal was underpinned by its robust preparation. Campaign CTO Harper Reed’s team systematically "role-played every possible disaster situation" to ensure the system’s resilience.6 This discipline proved essential when genuine technical crises occurred shortly before the election, including an Amazon Web Services outage and the physical threat posed by Hurricane Sandy to technology infrastructure.6 This focus on readiness demonstrates that the highest value of AI and analytics in complex operations is often related to stabilization and reliability rather than pure innovation. The 2012 outcome was ultimately a victory for data science discipline and engineering.
The campaign’s predictive models, fueled by big data and poll aggregation, predicted the outcome with remarkable accuracy, defying the general perception that the election would be a "nail biter".7 Quants and statisticians predicted Obama’s victory with close to 99% certainty in the final days, with some correctly predicting the exact electoral college vote count and identifying key swing states that would fall to Romney.7 This accurate modeling allowed the campaign to execute sophisticated cost-benefit analyses, avoiding panicked strategic shifts. For example, analytical models demonstrated that highly visible "swings" in support often reported by media polling were largely artifacts of variation in sample composition, not genuine shifts in voter intention, a crucial distinction in a highly polarized environment.9


2.2 Comparative Analysis: Narwhal vs. Project ORCA


The rival Mitt Romney campaign attempted to counter this analytical strategy with Project ORCA, a GOTV (Get Out The Vote) system whose name referenced the orca as the predator of the narwhal.10 Romney campaign officials boasted about their ground operation, claiming it was superior to Obama's.10
However, the contrast between the operational outcomes is stark. Narwhal’s seamless integration and rigorous testing proved robust, securing its functionality on Election Day. In contrast, Project ORCA suffered a catastrophic IT meltdown on Election Day, rendering the system largely unusable when it was most needed.6
This failure demonstrated that the primary competitive advantage of advanced analytics is derived from efficiency and technical architecture, not just the volume of data collected. As analysts noted regarding the saturated political market, the marginal impact of an additional generic auto-call is negligible; the structural advantage came from knowing precisely where to direct that finite resource for maximum return.5 The 2012 election was thus decided by superior technology architecture and risk mitigation, demonstrating that AI-powered insights win primarily by optimizing the allocation of scarce human and financial capital.
The subsequent table summarizes the operational and strategic distinctions between the two pivotal campaigns:
Table 1: Comparative Analysis of Data-Driven Election Strategies (2012 vs. 2016)


Campaign Cycle
	Key Technology/System
	Primary Strategic Goal
	Data Source Focus
	Mechanism for Victory
	Controversy/Risk
	Obama 2012
	Project Narwhal (Predictive Analytics) 6
	Operational Efficiency (GOTV, Resource Allocation) 5
	Public records, internal campaign data, polling aggregation 7
	Superior infrastructure integration; optimized resource allocation and turnout 6
	System failure risk (e.g., ORCA) 10
	Trump 2016/CA
	Psychographic Microtargeting (OCEAN Model) 12
	Persuasion based on emotional/personality vulnerabilities 13
	Facebook "Likes," consumer data, public voter files 14
	Demonstrated efficacy of personality-tailored ads; localized marginal shifts 15
	Mass data privacy breaches; characterization as "psy-ops" 16
	________________


Section 3: Case Study II: The Psychographic Frontier—Cambridge Analytica and the 2016 Era


The strategic landscape fundamentally shifted in 2016, moving away from the operational optimization of 2012 and towards the deep, often controversial, art of psychological persuasion. This era was defined by the methods of the notorious political data analytics firm, Cambridge Analytica (CA).


3.1 Psychographic Profiling: Methodology and Data Acquisition


While previous campaigns relied on segmenting voters by informational demographics (e.g., age, income, education) 4, CA introduced segmentation based on psychographics—behavioral profiles derived from personality traits.12 This methodology often utilized models of human personality, such as the OCEAN model (Openness, Conscientiousness, Extroversion, Agreeableness, Neuroticism).12
The technical process relied on inferring these psychological traits from individuals' online behavior and personal data. Research conducted by psychologists, some of whom pioneered the original techniques, confirmed the efficacy of this approach. By analyzing Facebook "likes," researchers could evaluate a person's psychological traits more accurately than their coworkers using only 10 likes, and better than their close friends with 70 likes.17 This high-precision inference allowed CA to acquire data on millions of Facebook users for profiling.12
The core technological leap demonstrated during this period was not in prediction accuracy but in the scalability of persuasion. Studies now demonstrate the empirical effectiveness of personalized political ads tailored to individuals' personalities, showing them to be more effective than generic messages.13 Furthermore, with the rise of tools like Generative AI and Large Language Models (LLMs), it is now feasible to automatically generate and validate these highly personalized advertisements on a massive scale, turning a manual psychological insight into an automated manipulation tool.13


3.2 Assessing Effectiveness and Contested Wins (Brexit and Trump 2016)


CA prominently promoted its ostensible success in swaying voters during both the Brexit referendum and the 2016 Trump campaign.8 Studies support the claim that highly personalized microtargeting can produce a relatively larger persuasive impact compared to generic alternative messaging strategies.15
However, the true magnitude of CA’s impact on the overall election outcome remains highly contested among academics . Some research indicates that while targeted ads work well, highly granular "microtargeted" ads—based on several complex voter characteristics—do not necessarily provide an additional benefit beyond that achieved by simpler segmentation . This suggests that the firm's perceived success may have stemmed from effective basic targeting of key subgroups rather than the deep psychological profiling itself.1
This analytical distinction is crucial: though personalized messages are demonstrably effective at the individual level, their influence on the overall election outcome is often overstated, particularly when compared to deep structural factors like socioeconomic conditions, polarization, and party identification . Psychographic targeting provides a localized, marginal edge, but it does not alter the fundamental political forces determining a national result. A winning strategy must therefore combine the operational magnitude of the 2012 model with the persuasive depth demonstrated in 2016.


3.3 The Ethical Fallout: Psy-Ops and Undermining Democratic Trust


The methods employed by CA led to significant legal and ethical fallout globally. Whistleblowers and subsequent public inquiries characterized CA’s actions as engaging in psychological operations, or "psy-ops," due to the coercive and deceptive nature of the targeting.16
The core ethical danger lies in the creation of a scalable "manipulation machine" that accelerates the distribution of biased content.13 A British Parliamentary committee concluded that this relentless targeting, which leverages individuals' "fears and the prejudices," is considered "more invasive than obviously false information" because it fundamentally restricts or diverts the views necessary for informed decision-making.8 This practice of exploiting unique vulnerabilities prevents voters from accessing the full spectrum of available information, thereby undermining trust in democratic institutions and weakening the foundations of free debate by creating detrimental echo chambers . The long-term cost of these types of "wins" is democratic decay, driven by cynicism and polarization.
________________


Section 4: The New AI Frontier: Generative Disruption and Systemic Threat (2024+)


The current electoral cycle marks a significant, and perhaps terrifying, inflection point. AI has transitioned from being merely analytical (2012) or purely persuasive (2016) to becoming fundamentally generative and disruptive.


4.1 Deepfakes and Deceptive Content Generation


Generative AI (GenAI) facilitates the rapid and cost-effective creation of highly realistic synthetic media—images, videos, and voices—the 'deepfakes' that threaten electoral integrity.11 This capacity for realistic deception poses an acute threat to electoral integrity.16
A high-profile instance occurred prior to the 2024 New Hampshire presidential primary, where an AI-generated robocall mimicked the voice of President Joe Biden, complete with his signature catchphrase.21 The call falsely advised thousands of registered voters to stay away from the polls, suggesting that participation in the primary would render them ineligible to vote in the general election.21 This tactic was a direct attempt at voter suppression and disenfranchisement, leveraging the perceived legitimacy of the president’s voice to mislead the public about electoral processes.16 This demonstrates a shift in strategic focus: winning elections using GenAI insights now involves leveraging these tools to disrupt the opponent's base or the administration of the election itself, moving the technology from a strategic campaign asset to a systemic risk factor.
This disruption is global, with AI-generated media playing roles ranging from nefarious to innocuous.6 In the 2023 Slovakian election, deepfakes defaming a political party leader circulated, potentially influencing the outcome in favor of a rival.6 Similar malicious attempts, such as the use of AI deepfakes to meddle in the Taiwanese election, underscore that this is an internationally scaled problem.6


4.2 AI in Campaign Operations and Administration


Beyond public disinformation, AI poses significant operational challenges to election administration. AI tools do not necessarily introduce categorically new cybersecurity risks, but they allow existing threats, such as social engineering, cyberattacks, and the spread of false information, to scale more quickly and effectively.2
For campaign staff and election officials, AI-generated text dramatically increases the success rate of phishing emails and other social engineering attacks, making it easier for bad actors to deceive victims into providing login credentials or installing malware.2 Furthermore, AI’s capacity to generate plausible, yet inaccurate, text and imagery is particularly harmful when dealing with critical voting information, such as dates, polling locations, and hours. This lack of accuracy can actively disenfranchise voters relying on AI-enhanced search or productivity tools.2
The most pressing challenge presented by GenAI is largely a communication challenge rather than a vulnerability in the underlying electoral process.21 The success factor in combating these threats relies heavily on non-technical responses. This includes the timely publicizing of corrective information, strengthening online safeguards, and, most critically, proactively building robust relationships between election officials, local journalists, and large technology companies.21 In the face of AI-fueled sabotage, the condition for "winning" shifts from generating highly effective content to effectively controlling the information flow and maintaining trust in official sources.
Table 2: The Evolving AI Threat Landscape (2024+)


Threat Category
	AI Mechanism
	Electoral Impact Pathway
	Real-World Example
	Policy Mitigation Focus
	Disinformation/Deception
	Generative AI/Deepfakes (Audio/Video)
	Misleading voters about candidate actions or positions 21
	New Hampshire Biden deepfake robocall (Voter Suppression) 21
	Technical verification, Disclosure laws, Prohibitions 11
	Systemic Sabotage
	Large Language Models (LLMs)
	Scaling sophisticated social engineering (phishing) against campaign staff 2
	Increased success rates of AI-generated phishing emails 2
	Cybersecurity training, Technical safeguards (CISA recommendations) 2
	Democratic Erosion
	Algorithmic Microtargeting
	Creating echo chambers; exploiting psychological vulnerabilities 13
	Automated personalized ads tailored to personality 13
	Information Fiduciary models, Data privacy regulation 4
	________________


Section 5: Legal, Ethical, and Regulatory Implications of AI-Powered Campaigns




5.1 The Regulatory Response: Prohibitions vs. Disclosures


The rapid spread of deepfakes has forced jurisdictions across the country to play regulatory catch-up. Currently, twenty-six US states have enacted laws targeting political deepfakes.11 These regulatory responses generally follow two distinct philosophical camps:
1. Prohibition: States like Minnesota and Texas prohibit the publication of political deepfakes within a certain number of days prior to an election.11 The goal of this approach is to prevent irreparable damage caused by malicious, misleading content published too late for effective rebuttal.
2. Disclosure: Twenty-four states require a clear disclosure on the political media, similar to financial disclosure requirements, stating that the content contains an AI-generated element or deepfake.11
However, the efficacy of disclosure laws is limited because malicious foreign or domestic operatives intent on sabotage or suppression will simply ignore the legal requirement to disclose.4 Regulation must focus less on forcing compliance and more on technical methods for detection and rapid quarantine. Forward-thinking states like Colorado and Utah have started mandating additional disclosures within the file’s metadata—the descriptive information about the creator and creation time—thereby creating a technical audit trail for verification and identification of provenance.11
Table 3: Regulatory Responses to AI in Elections (US State Overview)


Regulatory Approach
	Description
	Goal
	Challenges/Limitations
	Example States
	Prohibition
	Outright ban on the publication of political deepfakes within a specific timeframe before an election.
	Preventing last-minute, irreparable damage from false content.
	First Amendment conflicts; Difficulty of enforcement against foreign actors.
	Minnesota, Texas 11
	Disclosure Requirement
	Mandating that political media must contain a clear statement if it was generated or altered by AI.
	Informing the electorate; Providing transparency regarding content authenticity.
	Bad actors ignoring rules; Lack of standardization in definitions ("synthetic media").
	24 US states, including Wisconsin (separate law) 11
	Metadata Requirement
	Requiring disclosures embedded within the descriptive information of the digital file itself.
	Creating an auditable, technical trail for tracking media provenance.
	Requires platform cooperation; Technical sophistication required for compliance.
	Colorado, Utah 11
	

5.2 Ethical Imperatives and the Democratic Crisis


The rise of Big Data enables unprecedented individualized advertising, which poses profound ethical questions.4 The fundamental issue is that highly advanced technology allows for the creation of a manipulation capacity that targets individuals based on unique vulnerabilities without requiring direct human input.13 This capability should be an area of urgent concern for ethicists and policymakers.13
As revealed by investigations into Cambridge Analytica, the use of AI to target based on deep psychological fears is considered more invasive than simple false information.8 This use of precision targeting weakens the foundational democratic consensus by promoting cynicism, polarization, and echo chambers . The long-term damage of such techniques is the systematic erosion of democratic trust, a price that outweighs any marginal electoral gain.
To safeguard electoral integrity, policymakers must implement nuanced approaches that curb the most damaging impacts—such as suppression and fraud—without unduly restricting political expression.6 This necessitates developing legal frameworks that address the underlying data acquisition practices. The proposed solution of defining online service providers as "information fiduciaries" is vital, as it imposes special duties on these entities regarding data governance, thereby safeguarding the electorate's privacy and right to an informed decision.4
________________


Section 6: Conclusion: Lessons Learned for Winning Campaigns


Looking back across the last three electoral cycles, the road to victory through AI is not a single path but a strategic blend. The competitive advantage changes based on the technology at hand, but success is always dictated by how seamlessly that technology integrates into the core campaign mission.


6.1 Synthesis of Key Success Factors


The history of data-driven campaigns demonstrates a clear evolution in the source of competitive advantage:
1. 2012 Model (Operational Excellence): The primary mechanism for winning was superior predictive modeling combined with robust, integrated data infrastructure (Project Narwhal).6 Success was defined by Efficiency, Integration, and Risk Mitigation, ensuring the correct voters were mobilized at the optimal cost.5
2. 2016 Model (Persuasive Depth): Success was driven by the scalable, automated generation of personalized messages tailored to inferred psychological traits.13 This demonstrated the potential for Precision Persuasion to generate localized, marginal shifts in outcomes.15
3. 2024+ Model (Adaptive Defense): In the Generative AI era, a major factor in "winning" is the ability to Defend Against Disruption. Campaigns must master the strategy of countering rapid disinformation and voter suppression attempts without diverting excessive resources from core electoral activities.21


6.2 Future Outlook: Strategic Recommendations


Future winning campaigns must master a blend of these models, utilizing robust, 2012-style data integration and ethically sourced behavioral insights while simultaneously developing hardened defenses against AI-fueled sabotage.
The ongoing arms race requires campaigns to leverage AI for legitimate internal efficiency—such as drafting communications, optimizing scheduling, and streamlining logistics—while proactively developing comprehensive "playbooks" for responding to AI-driven threats.21 These defense strategies must emphasize fast, transparent communication and public dissemination of corrective information to mitigate the impact of GenAI-fueled disinformation. Given the limited resources available to election officials, state and federal governments must adequately fund partnership-building efforts across sectors to empower administrators to control the information environment and safeguard electoral integrity.21
Works cited
1. Don't Panic (Yet): Assessing the Evidence and Discourse Around Generative AI and Elections | Knight First Amendment Institute, accessed October 16, 2025, https://knightcolumbia.org/content/dont-panic-yet-assessing-the-evidence-and-discourse-around-generative-ai-and-elections
2. Artificial Intelligence (AI) and Election Administration - U.S. Election Assistance Commission, accessed October 16, 2025, https://www.eac.gov/AI
3. How Political Campaigns Use Your Data to Target You | Electronic Frontier Foundation, accessed October 16, 2025, https://www.eff.org/deeplinks/2024/04/how-political-campaigns-use-your-data-target-you
4. INFORMATION FIDUCIARIES AND POLITICAL MICROTARGETING: A LEGAL FRAMEWORK FOR REGULATING POLITICAL ADVERTISING ON DIGITAL PLATFORM - Scholarly Commons, accessed October 16, 2025, https://scholarlycommons.law.northwestern.edu/cgi/viewcontent.cgi?article=1460&context=nulr
5. Political Campaigns and Big Data - Harvard University, accessed October 16, 2025, https://scholar.harvard.edu/files/todd_rogers/files/nickerson_and_rogers.2014.pdf
6. Project Narwhal - Wikipedia, accessed October 16, 2025, https://en.wikipedia.org/wiki/Project_Narwhal
7. Big data, analytics and elections - PubsOnLine - Informs.org, accessed October 16, 2025, https://pubsonline.informs.org/do/10.1287/LYTX.2013.01.03/full/
8. persuasive effects of political microtargeting in the age of generative artificial intelligence | PNAS Nexus | Oxford Academic, accessed October 16, 2025, https://academic.oup.com/pnasnexus/article/3/2/pgae035/7591134
9. The Mythical Swing Voter - Columbia University, accessed October 16, 2025, https://sites.stat.columbia.edu/gelman/research/unpublished/swingers.pdf
10. ORCA (computer system) - Wikipedia, accessed October 16, 2025, https://en.wikipedia.org/wiki/ORCA_(computer_system)
11. Summary Artificial Intelligence (AI) in Elections and Campaigns, accessed October 16, 2025, https://www.ncsl.org/elections-and-campaigns/artificial-intelligence-ai-in-elections-and-campaigns
12. Psychographics: the behavioural analysis that helped Cambridge Analytica know voters' minds - IMD business school for management and leadership courses, accessed October 16, 2025, https://www.imd.org/research-knowledge/technology-management/articles/psychographics-the-behavioural-analysis-that-helped-cambridge-analytica-know-voters-minds/
13. The persuasive effects of political microtargeting in the age of generative artificial intelligence - PMC, accessed October 16, 2025, https://pmc.ncbi.nlm.nih.gov/articles/PMC10849795/
14. The Science Behind Cambridge Analytica: Does Psychological Profiling Work?, accessed October 16, 2025, https://www.gsb.stanford.edu/insights/science-behind-cambridge-analytica-does-psychological-profiling-work
15. Quantifying the potential persuasive returns to political microtargeting - PNAS, accessed October 16, 2025, https://www.pnas.org/doi/10.1073/pnas.2216261120
16. Psychological Operations in Digital Political Campaigns: Assessing Cambridge Analytica's Psychographic Profiling and Targeting - Frontiers, accessed October 16, 2025, https://www.frontiersin.org/journals/communication/articles/10.3389/fcomm.2020.00067/pdf
17. Political Campaigning, Microtargeting, and the Right to Information - Practical Ethics, accessed October 16, 2025, https://blog.practicalethics.ox.ac.uk/2024/01/political-campaigning-microtargeting-and-the-right-to-information/
18. Preparing for AI & Other Challenges to Election Administration | Bipartisan Policy Center, accessed October 16, 2025, https://bipartisanpolicy.org/report/preparing-for-artificial-intelligence-and-other-challenges-to-election-administration/
19. From mass networks to personalised voting - Diálogo Político, accessed October 16, 2025, https://dialogopolitico.org/special-edition-2025-artificial-democracy/from-mass-networks-to-personalised-voting
20. How Artificial Intelligence Influences Elections and What We Can Do About It, accessed October 16, 2025, https://campaignlegal.org/update/how-artificial-intelligence-influences-elections-and-what-we-can-do-about-it
21. CANDIDATE AI: THE IMPACT OF ARTIFICIAL INTELLIGENCE ON ELECTIONS, accessed October 16, 2025, https://news.emory.edu/features/2024/09/emag_ai_elections_25-09-2024/index.html